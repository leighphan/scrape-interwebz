{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Web APIs\n",
    "\n",
    "\n",
    "**Due**: November 3 at 4pm\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from urllib import quote_plus\n",
    "import json\n",
    "from __future__ import division\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: API Keys\n",
    "\n",
    "Get an API key from the [NYT Developer](http://developer.nytimes.com/apps/mykeys) website. Set your key in the variable given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set key\n",
    "key = \"ef9055ba947dd842effe0ecf5e338af9:15:72340235\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Requesting Data\n",
    "\n",
    "### 2.1 Edit the `get_api_year` function\n",
    "\n",
    "In the cell below, I've given you the code from lecture that defines a function which passes a search term (a string), and returns all articles mentioning that term in 2014. \n",
    "\n",
    "Edit this code so that is passes a second argument, `year` (an integer), and returns all the articles mentioning a a search term for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MAKE A FUNCTION HERE\n",
    "\n",
    "def get_api_data(term, year):\n",
    "    # set base url\n",
    "    base_url=\"http://api.nytimes.com/svc/search/v2/articlesearch\"\n",
    "\n",
    "    # set response format\n",
    "    response_format=\".json\"\n",
    "\n",
    "    # set search parameters\n",
    "    search_params = {\"q\":term,\n",
    "                 \"api-key\":key,\n",
    "                 \"begin_date\": str(year) + \"0101\", # date must be in YYYYMMDD format\n",
    "                 \"end_date\":str(year) + \"1231\"}\n",
    "\n",
    "    # make request\n",
    "    r = requests.get(base_url+response_format, params=search_params)\n",
    "    \n",
    "    # convert to a dictionary\n",
    "    data=json.loads(r.text)\n",
    "    \n",
    "    # get number of hits\n",
    "    hits = data['response']['meta']['hits']\n",
    "    print \"number of hits: \" + str(hits)\n",
    "    \n",
    "    # get number of pages\n",
    "    pages = int(math.ceil(hits/10))\n",
    "    \n",
    "    # make an empty list where we'll hold all of our docs for every page\n",
    "    all_docs = [] \n",
    "    \n",
    "    # now we're ready to loop through the pages\n",
    "    for i in range(pages):\n",
    "        print \"collecting page \" + str(i)\n",
    "        \n",
    "        # set the page parameter\n",
    "        search_params['page'] = i\n",
    "        \n",
    "        # make request\n",
    "        r = requests.get(base_url+response_format, params=search_params)\n",
    "    \n",
    "        # get text and convert to a dictionary\n",
    "        data=json.loads(r.text)\n",
    "        \n",
    "        # get just the docs\n",
    "        docs = data['response']['docs']\n",
    "        \n",
    "        # add those docs to the big list\n",
    "        all_docs = all_docs + docs\n",
    "        \n",
    "    return(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment to test\n",
    "# get_api_data(\"Duke Ellington\", 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create list of years\n",
    "\n",
    "In class, we collected all the articles mentioning Duke Ellington in 2014. Now we want to search over multiple years. Create a list called `years` that contains all the years from 2005 to 2014, inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = range(2005, 2015)\n",
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Collect data over multiple years\n",
    "\n",
    "Using the function you made in 2.1, loop over the list `years`, collecting data on articles that contain mention of \"Duke Ellington\". Store all years' data in an object called `all_duke`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hits: 77\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "number of hits: 101\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "collecting page 10\n",
      "number of hits: 111\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "collecting page 10\n",
      "collecting page 11\n",
      "number of hits: 99\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "number of hits: 114\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "collecting page 10\n",
      "collecting page 11\n",
      "number of hits: 94\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "number of hits: 95\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "number of hits: 66\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "number of hits: 115\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "collecting page 10\n",
      "collecting page 11\n",
      "number of hits: 171\n",
      "collecting page 0\n",
      "collecting page 1\n",
      "collecting page 2\n",
      "collecting page 3\n",
      "collecting page 4\n",
      "collecting page 5\n",
      "collecting page 6\n",
      "collecting page 7\n",
      "collecting page 8\n",
      "collecting page 9\n",
      "collecting page 10\n",
      "collecting page 11\n",
      "collecting page 12\n",
      "collecting page 13\n",
      "collecting page 14\n",
      "collecting page 15\n",
      "collecting page 16\n",
      "collecting page 17\n"
     ]
    }
   ],
   "source": [
    "all_duke = []\n",
    "for i in years:\n",
    "    all_duke.extend(get_api_data(\"Duke Ellington\", i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "len(all_duke) == 1043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': u'4fd2872b8eb7c8105d858553',\n",
       " u'abstract': None,\n",
       " u'blog': [],\n",
       " u'byline': {u'original': u'By N.R. Kleinfield',\n",
       "  u'person': [{u'firstname': u'N.',\n",
       "    u'lastname': u'Kleinfield',\n",
       "    u'middlename': u'R.',\n",
       "    u'organization': u'',\n",
       "    u'rank': 1,\n",
       "    u'role': u'reported'}]},\n",
       " u'document_type': u'article',\n",
       " u'headline': {u'kicker': u'New York Bookshelf',\n",
       "  u'main': u'NEW YORK BOOKSHELF/NONFICTION'},\n",
       " u'keywords': [{u'name': u'persons', u'value': u'ELLINGTON, DUKE'},\n",
       "  {u'name': u'persons', u'value': u'HARRIS, DANIEL'}],\n",
       " u'lead_paragraph': u\"A WIDOW'S WALK: A Memoir of 9/11 By Marian Fontana Simon & Schuster ($24, hardcover) Theresa and I walk into the Blue Ribbon, an expensive, trendy restaurant on Fifth Avenue in Park Slope. We sit at a banquette in the middle of the room and read the eclectic menu, my eyes instinctively scanning the prices for the least expensive item.\",\n",
       " u'multimedia': [],\n",
       " u'news_desk': u'The City Weekly Desk',\n",
       " u'print_page': u'9',\n",
       " u'pub_date': u'2005-10-02T00:00:00Z',\n",
       " u'section_name': u'New York and Region',\n",
       " u'slideshow_credits': None,\n",
       " u'snippet': u\"A WIDOW'S WALK:.\",\n",
       " u'source': u'The New York Times',\n",
       " u'subsection_name': None,\n",
       " u'type_of_material': u'News',\n",
       " u'web_url': u'http://www.nytimes.com/2005/10/02/nyregion/02bookshelf.html',\n",
       " u'word_count': 629}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_duke[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Formatting and Exporting\n",
    "\n",
    "### 3.1 Collect more fields\n",
    "\n",
    "In the cell below, I've pasted the code from lecture defining a function that accepts a list of unformatted documents returned by the API, and formats it into a clean list of dictionaries that contain keys for `id`, `headline`, and `date`.\n",
    "\n",
    "Edit the function so that we include the `lead_paragraph` and `word_count` fields.\n",
    "\n",
    "**HINT**: Some articles may not contain a lead_paragraph, in which case, it'll throw an error if you try to address this value (which doesn't exist.) You need to add a conditional statement that takes this into consideraiton. If\n",
    "\n",
    "**HINT**: Add `.encode(\"utf8\")` at the end of dictionary key lookups. You'll thank me later when you try to export your CSV.\n",
    "\n",
    "**Advanced**: Add another key that returns a list of `keywords` associated with the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_articles(unformatted_docs):\n",
    "    '''\n",
    "    This function takes in a list of documents returned by the NYT api \n",
    "    and parses the documents into a list of formated dictionaries, \n",
    "    with 'id', 'header', and 'date' keys\n",
    "    '''\n",
    "    formatted = []\n",
    "    for i in unformatted_docs:\n",
    "        dic = {}\n",
    "        dic['id'] = i['_id']\n",
    "        dic['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "        dic['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "        if i['lead_paragraph']:\n",
    "            dic['lead_paragraph'] = i['lead_paragraph'].encode(\"utf8\")\n",
    "        dic['word_count'] = i['word_count']\n",
    "        formatted.append(dic)\n",
    "    return(formatted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Format `all_duke`\n",
    "\n",
    "Using the function you made above, format the `all_duke` data. Store the result in an object called `all_duke_formatted`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_duke_formatted = format_articles(all_duke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': u'2005-10-02',\n",
       " 'headline': 'NEW YORK BOOKSHELF/NONFICTION',\n",
       " 'id': u'4fd2872b8eb7c8105d858553',\n",
       " 'lead_paragraph': \"A WIDOW'S WALK: A Memoir of 9/11 By Marian Fontana Simon & Schuster ($24, hardcover) Theresa and I walk into the Blue Ribbon, an expensive, trendy restaurant on Fifth Avenue in Park Slope. We sit at a banquette in the middle of the room and read the eclectic menu, my eyes instinctively scanning the prices for the least expensive item.\",\n",
       " 'word_count': 629}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test you code\n",
    "all_duke_formatted[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Export as CSV\n",
    "\n",
    "Export the object all_duke_formatted into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = all_duke_formatted[0]\n",
    "#writing the rest\n",
    "with open('allduke.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(all_duke_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extra Credit / Bonus / Advanced / Optional\n",
    "\n",
    "Import the data in R, and produce a graph that visualizes how Duke Ellington has changed in popularity over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See Assignment_7_R.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
