### STEP 4: Constructing API GET Requests in R

Because using Web APIs in R will involve repeatedly constructing different GET requests with slightly different components each time, it is helpful to store many of the individuals components as objects and combine them using ```paste()``` when ready to send the request.

In the first place, we know that every call will require us to provide a) a base URL for the API, b) some authorization code or key, and c) a format for the response.

```{r}
# Create objects holding the key, base url, and response format
key<-"ef9055ba947dd842effe0ecf5e338af9:15:72340235"
base.url<-"http://api.nytimes.com/svc/search/v2/articlesearch"
response.format<-".json"
```

Secondly, we need to specify our search terms, along with any filters to be placed on the results.  In this case, we are searching for the phrase "jazz is dead", though we specifically want it to appear in the body of the text.
```{r}
# Specify a main search term (q)
search.term<-"jazz is dead"

# Specify and encode filters (fc)
filter.query<-"body:\"jazz is dead\"" 
```

Note that it can often be tricky to properly re-format character strings stored in R objects to character strings suitable for GET requests.  For example, the filter above uses quotation marks to specify that we wanted to retrieve the phrase exactly.  But to include those quotation marks inside a character string that --- following R syntax --- must itself be surrounded by double quotation marks, these original characters need to be escaped with a backslash.  This results in the stored R string appearing to be different from the parsed R string.     
```{r}
# NOTE: double quotes within double quotes must be escaped with / so R can parse the character string
print(filter.query) # How R stores the string
cat(filter.query) # How R parses the string
```

To overcome some of these encoding issues, it is often helpful to URL encode our strings.  URL encoding basically translates punctuation marks, white space, and other non alphanumeric characters into a series of unique characters only recognizeable by URL decoders.  If you've ever seen %20 in a URL, this is actually a placeholder for a single space.  R provides helpful functions to doing this translation automatically.  
```{r}
# URL-encode the search and its filters
search.term<-URLencode(URL = search.term, reserved = TRUE)
filter.query<-URLencode(URL = filter.query, reserved = TRUE)
print(search.term)
print(filter.query)
```

Once all the pieces of our GET request are in place, we can use either the ```paste()``` or ```paste0()``` to combine a number of different character strings into a single character string.  This final string will be our URL for the GET request.
```{r}
# Paste components together to create URL for get request
get.request<-paste0(base.url, response.format, "?", "q=", search.term, "&fq=", filter.query, "&api-key=", key)
print(get.request)
```

Once we have the URL complete, we can send a properly formated GET request.  There are several packages that can do this, but ***httr*** provides a good balance of simplicity and reliability.  The main function of interest here is ```GET()```:
```{r}
# Send the GET request using httr package
response<-httr::GET(url = get.request)
print(response)
```

The ```content()``` function allows us to extract the html response in a format of our choosing (raw text, in this case):
```{r} 
# Inspect the content of the response, parsing the result as text
response<-httr::content(x = response, as = "text")
substr(x = response, start = 1, stop = 1000)
```

The final step in the process involves converting the results from JSON format to something easier to work with -- notably a data.frame.  The ***jsonlite*** package provides several easy conversion functions for moving between JSON and vectors, data.frames, and lists.
```{r}
# Convert JSON response to a dataframe
response.df<-jsonlite::fromJSON(txt = response, simplifyDataFrame = TRUE, flatten = TRUE)

# Inspect the dataframe
str(response.df, max.level = 3)

# Get number of hits
print(response.df$response$meta$hits)
```

Of course, most experiences using Web APIs will require *multiple* GET requests, each different from the next.  To speed this process along, we can create a function that can take several arguments and then automatically generate a properly formated GET request URL.  Here, for instance, is one such function we might write:
```{r}
# Write a function to create get requests
nytapi<-function(search.terms=NULL, begin.date=NULL, end.date=NULL, page=NULL,
                     base.url="http://api.nytimes.com/svc/search/v2/articlesearch",
                     response.format=".json",
                     key="ef9055ba947dd842effe0ecf5e338af9:15:72340235"){

  # Combine parameters
  params<-list(
    c("q", search.terms),
    c("begin_date", begin.date),
    c("end_date", end.date),
    c("page", page)
  )
  params<-params[sapply(X = params, length)>1]
  params<-sapply(X = params, FUN = paste0, collapse="=")
  params<-paste0(params, collapse="&")
  
  # URL encode query portion
  query<-URLencode(URL = params, reserved = FALSE)

  # Combine with base url and other options
  get.request<-paste0(base.url, response.format, "?", query, "&api-key=", key)
  
  # Send GET request
  response<-httr::GET(url = get.request)
  
  # Parse response to JSON
  response<-httr::content(response, "text")  
  response<-jsonlite::fromJSON(txt = response, simplifyDataFrame = T, flatten = T)
  
  return(response)
}
```

Now that we have our handy NYT API function, let's try and do some data analysis.  To figure out whether Duke Ellington is "trending" over the past few years, we can start by using our handy function to get a count of how often the New York Times mentions the Duke...
 
```{r}
# Get number of hits, number of page queries
duke<-nytapi(search.terms = "duke ellington", begin.date = 20050101, end.date = 20150101)
hits<-duke$response$meta$hits
print(hits)
round(hits/10)
```

After making a quick call to the API, it appears that we have a total of 1059 hits.  Since the API only allows us to download 10 results at a time, we need to make 106 calls! 
```{r}
# Get all articles   
duke.articles<-sapply(X = 0:105, FUN = function(page){
  #cat(page, "")
  response<-tryCatch(expr = {
    r<-nytapi(search.terms = "duke ellington", begin.date = 20050101, end.date = 20150101, page = page)
    r$response$docs
  }, error=function(e) NULL)
  return(response)
})

# Combine list of dataframes
duke.articles<-duke.articles[!sapply(X = duke.articles, FUN = is.null)]
duke.articles<-plyr::rbind.fill(duke.articles)
```

To figure out how Duke's popularity is changing over time, all we need to do is add an indicator for the year and month each article was published in, and then use the ***plyr*** package to count how many articles appear with each year-month combination:
```{r}
# Add year-month indicators
duke.articles$year.month<-format(as.Date(duke.articles$pub_date), "%Y-%m")
duke.articles$year.month<-as.Date(paste0(duke.articles$year.month, "-01"))

# Count articles per month
library(plyr)
duke.permonth<-ddply(.data = duke.articles, .variables = "year.month", summarize, count=length(year.month))

# Plot the trend over time
library(ggplot2)
ggplot(data = duke.permonth, aes(x = year.month, y = count))+geom_point()+geom_smooth(se=F)+
  theme_bw()+xlab(label = "Date")+ylab(label = "Article Count")+ggtitle(label = "Coverage of Duke Ellington")
```

Looks like he actually *is* getting more popular of late!












